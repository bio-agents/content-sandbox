{
    "additionDate": "2021-01-18T12:05:57Z",
    "bioagentsCURIE": "bioagents:dualvd",
    "bioagentsID": "dualvd",
    "confidence_flag": "agent",
    "description": "Dual Encoding Visual Dialogue (DualVD) is a library, which is able to adaptively select question-relevant information from the visual and semantic views in a hierarchical mode.",
    "editPermission": {
        "type": "private"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Image analysis",
                    "uri": "http://edamontology.org/operation_3443"
                },
                {
                    "term": "Visualisation",
                    "uri": "http://edamontology.org/operation_0337"
                }
            ]
        }
    ],
    "homepage": "https://github.com/JXZe/Learning_DualVD",
    "language": [
        "Python"
    ],
    "lastUpdate": "2021-03-03T20:46:19Z",
    "name": "DualVD",
    "owner": "Kigaard",
    "publication": [
        {
            "doi": "10.1109/TIP.2020.3034494",
            "metadata": {
                "abstract": "Different from Visual Question Answering task that requires to answer only one question about an image, Visual Dialogue task involves multiple rounds of dialogues which cover a broad range of visual content that could be related to any objects, relationships or high-level semantics. Thus one of the key challenges in Visual Dialogue task is to learn a more comprehensive and semantic-rich image representation that can adaptively attend to the visual content referred by variant questions. In this paper, we first propose a novel scheme to depict an image from both visual and semantic views. Specifically, the visual view aims to capture the appearance-level information in an image, including objects and their visual relationships, while the semantic view enables the agent to understand high-level visual semantics from the whole image to the local regions. Furthermore, on top of such dual-view image representations, we propose a Dual Encoding Visual Dialogue (DualVD) module, which is able to adaptively select question-relevant information from the visual and semantic views in a hierarchical mode. To demonstrate the effectiveness of DualVD, we propose two novel visual dialogue models by applying it to the Late Fusion framework and Memory Network framework. The proposed models achieve state-of-the-art results on three benchmark datasets. A critical advantage of the DualVD module lies in its interpretability. We can analyze which modality (visual or semantic) has more contribution in answering the current question by explicitly visualizing the gate values. It gives us insights in understanding of information selection mode in the Visual Dialogue task. The code is available at https://github.com/JXZe/Learning_DualVD.",
                "authors": [
                    {
                        "name": "Hu Y."
                    },
                    {
                        "name": "Jiang X."
                    },
                    {
                        "name": "Qin Z."
                    },
                    {
                        "name": "Wu Q."
                    },
                    {
                        "name": "Yu J."
                    },
                    {
                        "name": "Zhang W."
                    }
                ],
                "date": "2021-01-01T00:00:00Z",
                "journal": "IEEE transactions on image processing : a publication of the IEEE Signal Processing Society",
                "title": "Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue"
            },
            "pmid": "33141670"
        }
    ],
    "agentType": [
        "Script"
    ],
    "topic": [
        {
            "term": "Imaging",
            "uri": "http://edamontology.org/topic_3382"
        }
    ]
}
