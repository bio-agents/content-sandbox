{
    "accessibility": "Open access",
    "additionDate": "2022-06-28T18:40:38.486455Z",
    "bioagentsCURIE": "bioagents:mmgl",
    "bioagentsID": "mmgl",
    "confidence_flag": "agent",
    "cost": "Free of charge",
    "credit": [
        {
            "name": "Shuai Zheng"
        },
        {
            "name": "Yao Zhao"
        }
    ],
    "description": "an end-to-end Multi-modal Graph Learning framework (MMGL) for disease prediction with multi-modality",
    "editPermission": {
        "type": "public"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Aggregation",
                    "uri": "http://edamontology.org/operation_3436"
                }
            ]
        }
    ],
    "homepage": "https://github.com/SsGood/MMGL",
    "language": [
        "Python"
    ],
    "lastUpdate": "2022-06-28T18:40:38.489010Z",
    "license": "MIT",
    "name": "MMGL",
    "owner": "Chan019",
    "publication": [
        {
            "doi": "10.1109/TMI.2022.3159264",
            "metadata": {
                "abstract": "IEEEBenefiting from the powerful expressive capability of graphs, graph-based approaches have been popularly applied to handle multi-modal medical data and achieved impressive performance in various biomedical applications. For disease prediction tasks, most existing graph-based methods tend to define the graph manually based on specified modality (e.g., demographic information), and then integrated other modalities to obtain the patient representation by Graph Representation Learning (GRL). However, constructing an appropriate graph in advance is not a simple matter for these methods. Meanwhile, the complex correlation between modalities is ignored. These factors inevitably yield the inadequacy of providing sufficient information about the patient&#x2019;s condition for a reliable diagnosis. To this end, we propose an end-to-end Multi-modal Graph Learning framework (MMGL) for disease prediction with multi-modality. To effectively exploit the rich information across multi-modality associated with the disease, modality-aware representation learning is proposed to aggregate the features of each modality by leveraging the correlation and complementarity between the modalities. Furthermore, instead of defining the graph manually, the latent graph structure is captured through an effective way of adaptive graph learning. It could be jointly optimized with the prediction model, thus revealing the intrinsic connections among samples. Our model is also applicable to the scenario of inductive learning for those unseen data. An extensive group of experiments on two disease prediction tasks demonstrates that the proposed MMGL achieves more favorable performance. The code of MMGL is available at https://github.com/SsGood/MMGL.",
                "authors": [
                    {
                        "name": "Guo Z."
                    },
                    {
                        "name": "Liu Y."
                    },
                    {
                        "name": "Liu Z."
                    },
                    {
                        "name": "Yang Y."
                    },
                    {
                        "name": "Zhao Y."
                    },
                    {
                        "name": "Zheng S."
                    },
                    {
                        "name": "Zhu Z."
                    }
                ],
                "date": "2022-01-01T00:00:00Z",
                "journal": "IEEE Transactions on Medical Imaging",
                "title": "Multi-modal Graph Learning for Disease Prediction"
            },
            "pmid": "35286257"
        }
    ],
    "agentType": [
        "Workbench"
    ],
    "topic": [
        {
            "term": "Medical imaging",
            "uri": "http://edamontology.org/topic_3384"
        },
        {
            "term": "Pathology",
            "uri": "http://edamontology.org/topic_0634"
        }
    ]
}
