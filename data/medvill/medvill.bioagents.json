{
    "accessibility": "Open access",
    "additionDate": "2022-11-09T17:35:42.260946Z",
    "bioagentsCURIE": "bioagents:medvill",
    "bioagentsID": "medvill",
    "confidence_flag": "agent",
    "cost": "Free of charge",
    "credit": [
        {
            "name": "Edward Choi"
        },
        {
            "name": "Jong Hak Moon",
            "orcidid": "https://orcid.org/0000-0002-6708-3918"
        }
    ],
    "description": "Medical Vision Language Learner (MedViLL), which adopts a BERT-based architecture combined with a novel multi-modal attention masking scheme to maximize generalization performance for both vision-language understanding tasks (diagnosis classification, medical image-report retrieval, medical visual question answering) and vision-language generation task (radiology report generation).",
    "editPermission": {
        "type": "public"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Data retrieval",
                    "uri": "http://edamontology.org/operation_2422"
                }
            ]
        }
    ],
    "homepage": "https://github.com/SuperSupermoon/MedViLL",
    "language": [
        "Python"
    ],
    "lastUpdate": "2022-11-09T17:35:42.265363Z",
    "license": "MIT",
    "name": "MedViLL",
    "owner": "Chan019",
    "publication": [
        {
            "doi": "10.1109/JBHI.2022.3207502",
            "metadata": {
                "abstract": "IEEERecently a number of studies demonstrated impressive performance on diverse vision-language multi-modal tasks such as image captioning and visual question answering by extending the BERT architecture with multi-modal pre-training objectives. In this work we explore a broad set of multi-modal representation learning tasks in the medical domain, specifically using radiology images and the unstructured report. We propose Medical Vision Language Learner (MedViLL), which adopts a BERT-based architecture combined with a novel multi-modal attention masking scheme to maximize generalization performance for both vision-language understanding tasks (diagnosis classification, medical image-report retrieval, medical visual question answering) and vision-language generation task (radiology report generation). By statistically and rigorously evaluating the proposed model on four downstream tasks with three radiographic image-report datasets (MIMIC-CXR, Open-I, and VQA-RAD), we empirically demonstrate the superior downstream task performance of MedViLL against various baselines, including task-specific architectures. The source code is publicly available at: <uri>https://github.com/SuperSupermoon/MedViLL</uri>",
                "authors": [
                    {
                        "name": "Choi E."
                    },
                    {
                        "name": "Kim Y."
                    },
                    {
                        "name": "Lee H."
                    },
                    {
                        "name": "Moon J.H."
                    },
                    {
                        "name": "Shin W."
                    }
                ],
                "date": "2022-01-01T00:00:00Z",
                "journal": "IEEE Journal of Biomedical and Health Informatics",
                "title": "Multi-modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training"
            },
            "pmid": "36121943"
        }
    ],
    "agentType": [
        "Script"
    ],
    "topic": [
        {
            "term": "Medical imaging",
            "uri": "http://edamontology.org/topic_3384"
        }
    ]
}
