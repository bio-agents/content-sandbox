{
    "additionDate": "2021-01-18T11:38:00Z",
    "bioagentsCURIE": "bioagents:ldagan",
    "bioagentsID": "ldagan",
    "confidence_flag": "agent",
    "description": "Latent Dirichlet allocation based generative adversarial networks.\n\nLatent Dirichlet Allocation in Generative Adversarial Network.\n\nCode for the image generation experiments in Latent Dirichlet Allocation in Generative Adversarial Network.",
    "editPermission": {
        "type": "private"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Visualisation",
                    "uri": "http://edamontology.org/operation_0337"
                }
            ]
        }
    ],
    "homepage": "https://github.com/Sumching/LDAGAN",
    "language": [
        "Python"
    ],
    "lastUpdate": "2021-02-12T11:23:15Z",
    "name": "LDAGAN",
    "owner": "Niclaskn",
    "publication": [
        {
            "doi": "10.1016/J.NEUNET.2020.08.012",
            "metadata": {
                "abstract": "Â© 2020Generative adversarial networks have been extensively studied in recent years and powered a wide range of applications, ranging from image generation, image-to-image translation, to text-to-image generation, and visual recognition. These methods typically model the mapping from latent space to image with single or multiple generators. However, they have obvious drawbacks: (i) ignoring the multi-modal structure of images, and (ii) lacking model interpretability. Importantly, the existing methods mostly assume one or more generators can cover all image modes even if we do not know the structure of data. Thus, mode dropping and collapse often take place along with GANs training. Despite the importance of exploring the data structure in generation, it has been almost unexplored. In this work, aiming at generating multi-modal images and interpreting model explicitly, we explore the theory on how to integrate GANs with data structure prior, and propose latent Dirichlet allocation based generative adversarial networks (LDAGAN). This framework is extended to combine with a variety of state-of-the-art single-generator GANs and achieves improved performance. Extensive experiments on synthetic and real datasets demonstrate the efficacy of LDAGAN for multi-modal image generation. An implementation of LDAGAN is available at https://github.com/Sumching/LDAGAN.",
                "authors": [
                    {
                        "name": "Cheng S."
                    },
                    {
                        "name": "Liu J."
                    },
                    {
                        "name": "Pan L."
                    },
                    {
                        "name": "Ren Y."
                    },
                    {
                        "name": "Tang P."
                    },
                    {
                        "name": "Wang B."
                    },
                    {
                        "name": "Xu Z."
                    }
                ],
                "citationCount": 1,
                "date": "2020-12-01T00:00:00Z",
                "journal": "Neural Networks",
                "title": "Latent Dirichlet allocation based generative adversarial networks"
            },
            "pmid": "33039785"
        }
    ],
    "agentType": [
        "Command-line agent"
    ],
    "topic": [
        {
            "term": "Imaging",
            "uri": "http://edamontology.org/topic_3382"
        },
        {
            "term": "Mapping",
            "uri": "http://edamontology.org/topic_0102"
        },
        {
            "term": "Statistics and probability",
            "uri": "http://edamontology.org/topic_2269"
        }
    ]
}
