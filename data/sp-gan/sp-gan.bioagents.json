{
    "additionDate": "2021-01-18T10:03:52Z",
    "bioagentsCURIE": "bioagents:sp-gan",
    "bioagentsID": "sp-gan",
    "confidence_flag": "agent",
    "description": "Self-Growing and Pruning Generative Adversarial Networks.\n\nSP-GAN: Self-growing and Pruning GenerativeAdversarial Networks.\n\nThis paper presents a new Self-growing and Pruning Generative Adversarial Network (SP-GAN) for realistic image generation.",
    "editPermission": {
        "type": "private"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Network analysis",
                    "uri": "http://edamontology.org/operation_3927"
                }
            ]
        }
    ],
    "homepage": "https://github.com/Lambert-chen/SPGAN.git",
    "language": [
        "Python"
    ],
    "lastUpdate": "2021-02-20T10:11:01Z",
    "name": "SP-GAN",
    "owner": "zsmag19",
    "publication": [
        {
            "doi": "10.1109/TNNLS.2020.3005574",
            "metadata": {
                "abstract": "Â© 2021 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.This article presents a new Self-growing and Pruning Generative Adversarial Network (SP-GAN) for realistic image generation. In contrast to traditional GAN models, our SP-GAN is able to dynamically adjust the size and architecture of a network in the training stage by using the proposed self-growing and pruning mechanisms. To be more specific, we first train two seed networks as the generator and discriminator; each contains a small number of convolution kernels. Such small-scale networks are much easier and faster to train than large-capacity networks. Second, in the self-growing step, we replicate the convolution kernels of each seed network to augment the scale of the network, followed by fine-tuning the augmented/expanded network. More importantly, to prevent the excessive growth of each seed network in the self-growing stage, we propose a pruning strategy that reduces the redundancy of an augmented network, yielding the optimal scale of the network. Finally, we design a new adaptive loss function that is treated as a variable loss computational process for the training of the proposed SP-GAN model. By design, the hyperparameters of the loss function can dynamically adapt to different training stages. Experimental results obtained on a set of data sets demonstrate the merits of the proposed method, especially in terms of the stability and efficiency of network training. The source code of the proposed SP-GAN method is publicly.",
                "authors": [
                    {
                        "name": "Chen Y."
                    },
                    {
                        "name": "Feng Z.-H."
                    },
                    {
                        "name": "Hu G."
                    },
                    {
                        "name": "Song X."
                    },
                    {
                        "name": "Wu X.-J."
                    },
                    {
                        "name": "Yu D.-J."
                    }
                ],
                "citationCount": 1,
                "date": "2021-06-01T00:00:00Z",
                "journal": "IEEE Transactions on Neural Networks and Learning Systems",
                "title": "SP-GAN: Self-Growing and Pruning Generative Adversarial Networks"
            },
            "pmid": "32649282"
        }
    ],
    "topic": [
        {
            "term": "Imaging",
            "uri": "http://edamontology.org/topic_3382"
        }
    ]
}
