{
    "additionDate": "2021-11-06T10:25:36.641883Z",
    "bioagentsCURIE": "bioagents:epruner",
    "bioagentsID": "epruner",
    "confidence_flag": "agent",
    "credit": [
        {
            "name": "Mingbao Lin",
            "orcidid": "https://orcid.org/0000-0003-1764-1894"
        }
    ],
    "description": "Network Pruning Using Adaptive Exemplar Filters.",
    "editPermission": {
        "type": "public"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Filtering",
                    "uri": "http://edamontology.org/operation_3695"
                }
            ]
        }
    ],
    "homepage": "https://github.com/lmbxmu/EPruner",
    "language": [
        "Python"
    ],
    "lastUpdate": "2021-11-06T10:25:36.644487Z",
    "name": "EPruner",
    "operatingSystem": [
        "Linux",
        "Mac",
        "Windows"
    ],
    "owner": "Chan019",
    "publication": [
        {
            "doi": "10.1109/TNNLS.2021.3084856",
            "metadata": {
                "abstract": "IEEEPopular network pruning algorithms reduce redundant information by optimizing hand-crafted models, and may cause suboptimal performance and long time in selecting filters. We innovatively introduce adaptive exemplar filters to simplify the algorithm design, resulting in an automatic and efficient pruning approach called EPruner. Inspired by the face recognition community, we use a message-passing algorithm Affinity Propagation on the weight matrices to obtain an adaptive number of exemplars, which then act as the preserved filters. EPruner breaks the dependence on the training data in determining the ``important'' filters and allows the CPU implementation in seconds, an order of magnitude faster than GPU-based SOTAs. Moreover, we show that the weights of exemplars provide a better initialization for the fine-tuning. On VGGNet-16, EPruner achieves a 76.34&#x0025;-FLOPs reduction by removing 88.80&#x0025; parameters, with 0.06&#x0025; accuracy improvement on CIFAR-10. In ResNet-152, EPruner achieves a 65.12&#x0025;-FLOPs reduction by removing 64.18&#x0025; parameters, with only 0.71&#x0025; top-5 accuracy loss on ILSVRC-2012. Our code is available at https://github.com/lmbxmu/EPruner.",
                "authors": [
                    {
                        "name": "Huang F."
                    },
                    {
                        "name": "Ji R."
                    },
                    {
                        "name": "Li S."
                    },
                    {
                        "name": "Lin M."
                    },
                    {
                        "name": "Wang Y."
                    },
                    {
                        "name": "Wu Y."
                    },
                    {
                        "name": "Ye Q."
                    }
                ],
                "date": "2021-01-01T00:00:00Z",
                "journal": "IEEE Transactions on Neural Networks and Learning Systems",
                "title": "Network Pruning Using Adaptive Exemplar Filters"
            },
            "pmid": "34101606"
        }
    ],
    "agentType": [
        "Script"
    ],
    "topic": [
        {
            "term": "Computer science",
            "uri": "http://edamontology.org/topic_3316"
        }
    ]
}
