{
    "accessibility": "Open access",
    "additionDate": "2022-10-28T06:27:07.402862Z",
    "bioagentsCURIE": "bioagents:ccldnet",
    "bioagentsID": "ccldnet",
    "confidence_flag": "agent",
    "cost": "Free of charge",
    "credit": [
        {
            "name": "Huakun Yang"
        },
        {
            "name": "Yanye Lu"
        }
    ],
    "description": "Boosting medical image segmentation via conditional-synergistic convolution and lesion decoupling.",
    "editPermission": {
        "type": "public"
    },
    "homepage": "https://github.com/QianChen98/CCLD-Net",
    "language": [
        "Python"
    ],
    "lastUpdate": "2022-10-28T06:27:07.405392Z",
    "license": "MIT",
    "name": "CCLDNet",
    "operatingSystem": [
        "Linux"
    ],
    "owner": "Chan019",
    "publication": [
        {
            "doi": "10.1016/J.COMPMEDIMAG.2022.102110",
            "metadata": {
                "abstract": "Â© 2022 Elsevier LtdMedical image segmentation is a critical step in pathology assessment and monitoring. Extensive methods tend to utilize a deep convolutional neural network for various medical segmentation tasks, such as polyp segmentation, skin lesion segmentation, etc. However, due to the inherent difficulty of medical images and tremendous data variations, they usually perform poorly in some intractable cases. In this paper, we propose an input-specific network called conditional-synergistic convolution and lesion decoupling network (CCLDNet) to solve these issues. First, in contrast to existing CNN-based methods with stationary convolutions, we propose the conditional synergistic convolution (CSConv) that aims to generate a specialist convolution kernel for each lesion. CSConv has the ability of dynamic modeling and could be leveraged as a basic block to construct other networks in a broad range of vision tasks. Second, we devise a lesion decoupling strategy (LDS) to decouple the original lesion segmentation map into two soft labels, i.e., lesion center label and lesion boundary label, for reducing the segmentation difficulty. Besides, we use a transformer network as the backbone, further erasing the fixed structure of the standard CNN and empowering dynamic modeling capability of the whole framework. Our CCLDNet outperforms state-of-the-art approaches by a large margin on a variety of benchmarks, including polyp segmentation (89.22% dice score on EndoScene) and skin lesion segmentation (91.15% dice score on ISIC2018). Our code is available at https://github.com/QianChen98/CCLD-Net.",
                "authors": [
                    {
                        "name": "Chen Q."
                    },
                    {
                        "name": "Du H."
                    },
                    {
                        "name": "Fu K."
                    },
                    {
                        "name": "Jin L."
                    },
                    {
                        "name": "Lu Y."
                    },
                    {
                        "name": "Qiu B."
                    },
                    {
                        "name": "Ren Q."
                    },
                    {
                        "name": "Yang H."
                    },
                    {
                        "name": "Zhu L."
                    }
                ],
                "date": "2022-10-01T00:00:00Z",
                "journal": "Computerized Medical Imaging and Graphics",
                "title": "Boosting medical image segmentation via conditional-synergistic convolution and lesion decoupling"
            },
            "pmid": "36057184"
        }
    ],
    "agentType": [
        "Command-line agent"
    ],
    "topic": [
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        },
        {
            "term": "Medical imaging",
            "uri": "http://edamontology.org/topic_3384"
        },
        {
            "term": "Pathology",
            "uri": "http://edamontology.org/topic_0634"
        }
    ]
}
